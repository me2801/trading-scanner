{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 30/30 Momentum test (monthly)\n\nWe test a simple cross-sectional momentum rule:\n\n- Signal at month **t**: **12M momentum excluding current month**  \n  `log(P[t-1] / P[t-13])`\n- Rank tickers by that signal and select:\n  - **Winners** = top 30%  → `selected = 1`\n  - **Losers**  = bottom 30% → `selected = -1`\n  - Others → `selected = 0`\n- Next-month return (for each asset): `log(P[t] / P[t-1])`\n\nPortfolio columns (log space, equal-weight within buckets):\n\n- `winner` = average log return of winners\n- `loser_long` = average log return of losers **if held long**\n- `wml_spread` = `winner - loser_long`\n- `wml_cap05` = `2 * wml_spread` (your “0.5 capital” convention)\n\nOutputs:\n- A wide Excel sheet (2 header rows) in `data/analysis/`\n- Decade summary table (first/last decades can be partial)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 0 — Locate repo root and ensure imports come from this repo\n\nThis avoids accidentally importing an old `site-packages` version while you’re editing code in `src/`.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport sys\n\ndef find_repo_root(start: Path | None = None) -> Path:\n    \"\"\"Walk upwards from `start` (or CWD) until we find a repo marker.\"\"\"\n    start = (start or Path.cwd()).resolve()\n    markers = (\"pyproject.toml\", \".git\", \"setup.cfg\", \"requirements.txt\")\n    for p in (start, *start.parents):\n        if any((p / m).exists() for m in markers):\n            return p\n    return start\n\nREPO_ROOT = find_repo_root()\n\n# Ensure the repo's src/ is imported (not an old site-packages install)\nSRC = REPO_ROOT / \"src\"\nif str(SRC) not in sys.path:\n    sys.path.insert(0, str(SRC))\n\nprint(\"CWD:\", Path.cwd())\nprint(\"REPO_ROOT:\", REPO_ROOT)\nprint(\"SRC:\", SRC)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1 — Load bars from SQLite via infra\n\nUses `SQLiteBarStore.list_bars_multi()` to bulk-load bars. Adjust the `tickers=` list if you want to restrict the universe.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nfrom scanner_bot.infra.db.sqllite_barstore import SQLiteBarStore\n\nDB_PATH = (REPO_ROOT / \"data\" / \"db\" / \"scanner.db\").resolve()\n\nstore = SQLiteBarStore(DB_PATH)\n\n# Option A: all tickers in DB (fine if DB isn't huge)\nbars = store.list_bars_multi(interval=\"1d\", tickers=None)\n\n# Option B: restrict to a list (uncomment)\n# tickers = [\"AAPL\", \"MSFT\", \"NVDA\"]\n# bars = store.list_bars_multi(interval=\"1d\", tickers=tickers)\n\ndef _px(b):\n    # prefer adj_close when present\n    return b.adj_close if getattr(b, \"adj_close\", None) is not None else b.close\n\ndf = pd.DataFrame(\n    {\n        \"date\": [b.date for b in bars],\n        \"ticker\": [b.ticker for b in bars],\n        \"px\": [_px(b) for b in bars],\n    }\n).dropna(subset=[\"px\"])\n\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf.head(), df[\"ticker\"].nunique(), df[\"date\"].min(), df[\"date\"].max()\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2 — Resample to month-end prices\n\nWe use the last available daily price each month as the month-end close.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "px_m = (\n    df.set_index(\"date\")\n      .groupby(\"ticker\")[\"px\"]\n      .resample(\"M\")\n      .last()\n      .unstack(\"ticker\")\n      .sort_index()\n)\n\nprint(\"Months:\", len(px_m), \"| Tickers:\", px_m.shape[1])\npx_m.tail()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3 — Parameters / sanity check\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nTOP_PCT = 0.30\nBOT_PCT = 0.30\nLOOKBACK_MONTHS = 12      # \"last 12 months\"\nEXCLUDE_CURRENT = True    # use Pt-1 / Pt-13\nMIN_TICKERS = 10          # skip months with too few names\n\n# must already exist\nassert \"px_m\" in globals(), \"Expected px_m (month-end price matrix) to exist.\"\npx_m = px_m.sort_index()\n\nprint(\"px_m shape:\", px_m.shape)\nprint(\"date range:\", px_m.index.min(), \"->\", px_m.index.max())\nprint(\"example tickers:\", list(px_m.columns[:10]))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4 — Log returns and 12M momentum excluding current month\n\n- Monthly log return: `log(P[t]/P[t-1])`\n- 12M momentum (exclude current month): `log(P[t-1]/P[t-13])`\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# log return for current month: log(Pt / Pt-1)\nlogret_1m = np.log(px_m / px_m.shift(1))\n\n# 12M momentum excluding current period: log(Pt-1 / Pt-13)\n# (Pt-1 is shift(1), Pt-13 is shift(13))\nmom_12m_excl = np.log(px_m.shift(1) / px_m.shift(13))\n\n# optional: for readability in excel\nret_1m_simple = np.exp(logret_1m) - 1\nmom_12m_simple = np.exp(mom_12m_excl) - 1\n\nlogret_1m.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5 — Winners/Losers selection and portfolio returns (log space)\n\nSelection size per month:\n- `k = floor(N * 30%)`\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def monthly_select_and_portfolio(\n    px_m: pd.DataFrame,\n    logret_1m: pd.DataFrame,\n    mom_12m_excl: pd.DataFrame,\n    top_pct: float = 0.30,\n    bot_pct: float = 0.30,\n    min_tickers: int = 10,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Returns:\n      panel: rows = (date, ticker) with price, returns, momentum, selection\n      port : rows = date with winners, losers (short contrib), wml, counts\n    \"\"\"\n    panel_rows = []\n    port_rows = []\n\n    for dt in px_m.index:\n        s = mom_12m_excl.loc[dt].dropna()\n        r = logret_1m.loc[dt].dropna()\n        p = px_m.loc[dt].dropna()\n\n        common = s.index.intersection(r.index).intersection(p.index)\n        n = len(common)\n        if n < min_tickers:\n            continue\n\n        k = int(np.floor(n * top_pct))\n        k2 = int(np.floor(n * bot_pct))\n        if k <= 0 or k2 <= 0:\n            continue\n\n        s = s.loc[common]\n        r = r.loc[common]\n        p = p.loc[common]\n\n        ranked = s.sort_values(ascending=False)\n        winners = ranked.index[:k]\n        losers  = ranked.index[-k2:]\n\n        # selection vector: 1 / 0 / -1\n        sel = pd.Series(0, index=common, dtype=int)\n        sel.loc[winners] = 1\n        sel.loc[losers] = -1\n\n        # portfolio log returns (equal weight)\n        winners_ret = float(r.loc[winners].mean())          # log return of winners (long)\n        losers_long = float(r.loc[losers].mean())           # log return of losers if held long\n\n        wml_spread = winners_ret - losers_long              # spread in log space (your current W-L)\n        wml_cap05  = 2.0 * wml_spread                       # “return on 0.5 capital” (your convention)\n\n        port_rows.append(\n            {\n                \"date\": dt,\n                \"n_tickers\": n,\n                \"k_winners\": k,\n                \"k_losers\": k2,\n                \"winners_logret\": winners_ret,\n                \"losers_long_logret\": losers_long,\n                \"wml_spread_logret\": wml_spread,\n                \"wml_cap05_logret\": wml_cap05,\n            }\n        )\n\n        # per-ticker panel rows for excel\n        out = pd.DataFrame(\n            {\n                \"date\": dt,\n                \"ticker\": common,\n                \"price\": p.values,\n                \"logret_1m\": r.values,\n                \"mom_12m_excl_log\": s.values,\n                \"selected\": sel.values,  # 1 / 0 / -1\n            }\n        )\n        panel_rows.append(out)\n\n    panel = pd.concat(panel_rows, ignore_index=True) if panel_rows else pd.DataFrame()\n    port = pd.DataFrame(port_rows).set_index(\"date\").sort_index() if port_rows else pd.DataFrame()\n    return panel, port\n\npanel, port = monthly_select_and_portfolio(\n    px_m=px_m,\n    logret_1m=logret_1m,\n    mom_12m_excl=mom_12m_excl,\n    top_pct=TOP_PCT,\n    bot_pct=BOT_PCT,\n    min_tickers=MIN_TICKERS,\n)\n\nprint(\"panel rows:\", len(panel), \"| portfolio months:\", len(port))\nport.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6 — Optional convenience columns + selection sanity checks\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Optional convenience columns (simple returns) + sanity checks\n\nif not panel.empty:\n    panel[\"ret_1m\"] = np.exp(panel[\"logret_1m\"]) - 1\n    panel[\"mom_12m_excl\"] = np.exp(panel[\"mom_12m_excl_log\"]) - 1\n\n# Sanity checks per month: number of selected winners/losers equals floor(N * 30%)\nif not panel.empty and not port.empty:\n    sel_counts = (\n        panel.groupby(\"date\")[\"selected\"]\n             .agg(\n                 winners=lambda x: int((x == 1).sum()),\n                 losers=lambda x: int((x == -1).sum()),\n             )\n    )\n    chk = port.join(sel_counts, how=\"left\")\n    chk[\"ok_winners\"] = chk[\"winners\"] == chk[\"k_winners\"]\n    chk[\"ok_losers\"] = chk[\"losers\"] == chk[\"k_losers\"]\n    print(chk[[\"k_winners\",\"winners\",\"ok_winners\",\"k_losers\",\"losers\",\"ok_losers\"]].tail(12))\n\n# Optional: convert leg log-returns to simple returns (these are true leg returns)\nif not port.empty:\n    port[\"winners_ret\"] = np.exp(port[\"winners_logret\"]) - 1\n    port[\"losers_long_ret\"] = np.exp(port[\"losers_long_logret\"]) - 1\n\nport.tail()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7 — Build the wide output table for Excel\n\nOne row per month, 2 header rows:\n\nTop header: `DATE | <TICKER> ... | PORTFOLIO`  \nSecond header: `price | logret_1m | logret_12m | selected` (per ticker) and `winner | loser_long | wml_cap05` (portfolio).\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# --- Wide output table with 2 header rows: (TICKER, metric) and (PORTFOLIO, metric) ---\n\n# Per-ticker blocks (MultiIndex columns: (ticker, metric))\nprice_w   = panel.pivot(index=\"date\", columns=\"ticker\", values=\"price\")\nlr1_w     = panel.pivot(index=\"date\", columns=\"ticker\", values=\"logret_1m\")\nlr12_w    = panel.pivot(index=\"date\", columns=\"ticker\", values=\"mom_12m_excl_log\")  # log(P[t-1]/P[t-13])\nsel_w     = panel.pivot(index=\"date\", columns=\"ticker\", values=\"selected\")\n\n# Build as (metric, ticker) then swap to (ticker, metric)\ntickers_block = pd.concat(\n    {\n        \"price\": price_w,\n        \"logret_1m\": lr1_w,\n        \"logret_12m\": lr12_w,\n        \"selected\": sel_w,\n    },\n    axis=1\n)\ntickers_block = tickers_block.swaplevel(0, 1, axis=1).sort_index(axis=1, level=0)\n\n# Portfolio block (PORTFOLIO, metric)\n# - loser_long is the losers' return if held long\n# - wml_cap05 is 2 * (winner - loser_long) (your 0.5-capital convention)\nport_cols = pd.DataFrame(\n    {\n        \"winner\": port[\"winners_logret\"],\n        \"loser_long\": port[\"losers_long_logret\"],\n        \"wml_cap05\": port[\"wml_cap05_logret\"],\n    },\n    index=port.index\n)\nport_cols.columns = pd.MultiIndex.from_product([[\"PORTFOLIO\"], port_cols.columns])\n\n# Combine\nout = tickers_block.join(port_cols, how=\"inner\").sort_index()\n\n# Add DATE as first column (kept for display; Excel export uses DATE as index due to pandas limitation)\nout = out.reset_index()\nout.columns = pd.MultiIndex.from_tuples([(\"DATE\", \"\")] + list(out.columns[1:]))\n\nout.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8 — Prepare Excel output path\n\nWe keep `DATE` as index on export (pandas limitation with MultiIndex columns and `index=False`).\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "analysis_dir = (REPO_ROOT / \"data\" / \"analysis\")\nanalysis_dir.mkdir(parents=True, exist_ok=True)\nout_path = analysis_dir / \"momentum_30_30_monthly_wide.xlsx\"\n\n# pandas can't write MultiIndex columns with index=False -> use DATE as index\nif (\"DATE\", \"\") in out.columns:\n    out_xl = out.set_index((\"DATE\", \"\"))\nelse:\n    out_xl = out.copy()\n\nout_xl.index.name = \"DATE\"\nout_path\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9 — Decade performance\n\nA decade starts on years ending with **0** (e.g., 1990–1999, 2000–2009).  \nFirst/last decades can be partial depending on your data range.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\n\ndef _max_dd_from_logrets(r_log: pd.Series) -> float:\n    r_log = r_log.dropna()\n    if r_log.empty:\n        return np.nan\n    equity = np.exp(r_log.cumsum())  # log-additive -> equity curve\n    dd = equity / equity.cummax() - 1.0\n    return float(dd.min())\n\ndef _stats_from_logrets(r_log: pd.Series) -> dict:\n    r_log = r_log.dropna()\n    if r_log.empty:\n        return {\n            \"months\": 0,\n            \"total_log\": np.nan,\n            \"total_return\": np.nan,\n            \"cagr\": np.nan,\n            \"vol\": np.nan,\n            \"sharpe\": np.nan,\n            \"maxdd\": np.nan,\n        }\n\n    months = len(r_log)\n    total_log = float(r_log.sum())\n    total_return = float(np.exp(total_log) - 1.0)\n\n    years = months / 12.0\n    cagr = float(np.exp(total_log / years) - 1.0) if years > 0 else np.nan\n\n    vol = float(r_log.std(ddof=1) * np.sqrt(12)) if months > 1 else np.nan\n    sharpe = float((r_log.mean() * 12) / (r_log.std(ddof=1) * np.sqrt(12))) if months > 1 else np.nan\n\n    maxdd = _max_dd_from_logrets(r_log)\n\n    return {\n        \"months\": months,\n        \"total_log\": total_log,\n        \"total_return\": total_return,\n        \"cagr\": cagr,\n        \"vol\": vol,\n        \"sharpe\": sharpe,\n        \"maxdd\": maxdd,\n    }\n\ndef decade_year(dt: pd.Timestamp) -> int:\n    # decades start at YYYY-01-01 where YYYY ends with 0\n    return (dt.year // 10) * 10\n\n# Ensure datetime index\nport2 = port.copy()\nport2.index = pd.to_datetime(port2.index)\n\n# Decade stats based on your log columns\ncols = [\"winners_logret\", \"losers_long_logret\", \"wml_spread_logret\", \"wml_cap05_logret\"]\n\ndec_rows = []\nfor dy, g in port2[cols].groupby(port2.index.map(decade_year)):\n    start = g.index.min().date()\n    end = g.index.max().date()\n\n    row = {\"decade_start_year\": dy, \"start\": start, \"end\": end}\n    for c in cols:\n        s = _stats_from_logrets(g[c])\n        prefix = c.replace(\"_logret\", \"\")\n        row.update({f\"{prefix}_{k}\": v for k, v in s.items()})\n    dec_rows.append(row)\n\ndecades = pd.DataFrame(dec_rows).sort_values(\"decade_start_year\").set_index(\"decade_start_year\")\ndecades\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10 — Write workbook (monthly + decades)\n\nThis overwrites the workbook at the same path, ensuring both sheets are included.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Write both the wide monthly sheet and the decade summary into one workbook.\nanalysis_dir = (REPO_ROOT / \"data\" / \"analysis\")\nanalysis_dir.mkdir(parents=True, exist_ok=True)\nout_path = analysis_dir / \"momentum_30_30_monthly_wide.xlsx\"\n\nassert \"out_xl\" in globals(), \"Expected out_xl (wide monthly table) to exist.\"\nassert \"decades\" in globals(), \"Expected decades table to exist.\"\n\nwith pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n    out_xl.to_excel(writer, sheet_name=\"monthly\", index=True)\n    decades.to_excel(writer, sheet_name=\"decades\", index=True)\n\n    ws = writer.sheets[\"monthly\"]\n    ws.freeze_panes = \"B3\"\n\nprint(\"Saved:\", out_path)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}